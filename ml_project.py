# -*- coding: utf-8 -*-
"""ML_Project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1j4vN5lKStbHypt063gLlVTcAeXRRGTik

# Outline

1) Problem

2) Import data

3) Visualize data (for funsies)

3) Data cleaning - Christine

4) Run through multiple models

5) Tune the selected model

6) Present your solution

# Problem: maybe title here or smth

# The Data
import data and a lil visualization of the correlations

Go to [spreadsheet](https://docs.google.com/spreadsheets/d/1YvNCkJEKMRMebIcpjLd1U2vu1tJ1K5iVmRVppe-rq-g/edit#gid=2025110937) > Download first sheet as csv and upload file to Colab under sample_data.
"""

# Libraries
import sklearn # general ml package
import pandas as pd # Pandas module to enable data manipulation
import numpy as np # fundamental package for scientific computing
import os # to run file I/O operation 
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split

# to make this notebook's output stable across runs
# any number will do, as long as it is used consistently
np.random.seed(42)

# Import data from COVID-19_HOI_Data__Charlottesville_only_ sheet.
DATA_PATH = "sample_data"

def load_data(data_path=DATA_PATH):
    """Load Data into Workspace from a CSV"""
    csv_path = os.path.join(data_path, "ML_project_spreadsheet - COVID-19_HOI_Data__Charlottesville_only.csv")
    return pd.read_csv(csv_path, delim_whitespace=False)

data = load_data()
# Display the first n rows of the data. (n=5 by default, we will use 10 rows)
data.head(10)

"""# Data Cleaning

Christine - clean the data
"""

# Drop non-numerical or relevant labels from the dataset:
drop_labels = [ 
               "admin2", "country_region", 
               "province_state", "last_update", "geocoded_column",
               "deaths_delta", "Unnamed: 16"
              ] 

data_num = data.drop(drop_labels, axis=1)
data_num.head(10)

# Split into X and Y training and test sets.
train_set, test_set = train_test_split(data_num, test_size=0.2, random_state=42)

# Pipeline data.
pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy="median")), 
        ('std_scaler', StandardScaler()),
    ])

train_set_p = pipeline.fit_transform(train_set) # train data transformed and prepared.
test_set_p = pipeline.fit_transform(test_set) # test data transformed and prepared.

# Split sets further
X_train = train_set.drop("deaths", axis=1)
y_train = train_set["deaths"].copy()
X_test = test_set.drop("deaths", axis=1)
y_test = test_set["deaths"].copy()

"""[HOI features](https://apps.vdh.virginia.gov/omhhe/hoi/what-is-the-hoi/definitions): Access to Care	Employment Accessibility	Affordability	Air Quality	Population Churning	Education	Food Accessibility	Income Inequality	Job Participation	Population Density	Segregation	Material Deprivation	Walkability	Community Environment Profile	Consumer Opportunity Profile	Economic Opportunity Profile	Wellness Disparity Profile	Health Opportunity Index. 	"""

### For reference - TODO: Delete later.
all_features = [
            "fips",	"lat",	"long_",	"confirmed",	"confirmed_delta",
            "recovered",	"recovered_delta",	"active",	"active_delta",
            "Access to Care", "Employment Accessibility",	"Affordability",
            "Air Quality",	"Population Churning", "Education",	"Food Accessibility",
            "Income Inequality",	"Job Participation",	"Population Density",
            "Segregation",	"Material Deprivation",	"Walkability",
            "Community Environment Profile",	"Consumer Opportunity Profile",
            "Economic Opportunity Profile",	"Wellness Disparity Profile",
            "Health Opportunity Index",
            ]

# [CONTROL] Sets without any HOI data 
X_train_noHOI = X_train.drop(["Access to Care", "Employment Accessibility",	"Affordability",
            "Air Quality",	"Population Churning", "Education",	"Food Accessibility",
            "Income Inequality",	"Job Participation",	"Population Density",
            "Segregation",	"Material Deprivation",	"Walkability",
            "Community Environment Profile",	"Consumer Opportunity Profile",
            "Economic Opportunity Profile",	"Wellness Disparity Profile",
            "Health Opportunity Index",], axis=1) 
X_test_noHOI = X_test.drop(["Access to Care", "Employment Accessibility",	"Affordability",
            "Air Quality",	"Population Churning", "Education",	"Food Accessibility",
            "Income Inequality",	"Job Participation",	"Population Density",
            "Segregation",	"Material Deprivation",	"Walkability",
            "Community Environment Profile",	"Consumer Opportunity Profile",
            "Economic Opportunity Profile",	"Wellness Disparity Profile",
            "Health Opportunity Index",], axis=1) 

# Sets without all HOI features but the HOI calculation itself.
X_train_health = X_train.drop(["Access to Care", "Employment Accessibility",	"Affordability",
            "Air Quality",	"Population Churning", "Education",	"Food Accessibility",
            "Income Inequality",	"Job Participation",	"Population Density",
            "Segregation",	"Material Deprivation",	"Walkability",
            "Community Environment Profile",	"Consumer Opportunity Profile",
            "Economic Opportunity Profile",	"Wellness Disparity Profile",], axis=1) 
X_test_health = X_test.drop(["Access to Care", "Employment Accessibility",	"Affordability",
            "Air Quality",	"Population Churning", "Education",	"Food Accessibility",
            "Income Inequality",	"Job Participation",	"Population Density",
            "Segregation",	"Material Deprivation",	"Walkability",
            "Community Environment Profile",	"Consumer Opportunity Profile",
            "Economic Opportunity Profile",	"Wellness Disparity Profile",], axis=1)

# Sets with the Community Environment profile: Air Quality, Population Churning, Population Density, Walkability
X_train_community = X_train.drop(["Access to Care", "Employment Accessibility",	"Affordability",
          	"Education",	"Food Accessibility",
            "Income Inequality",	"Job Participation",	"Population Density",
            "Segregation",	"Material Deprivation", "Consumer Opportunity Profile",
            "Economic Opportunity Profile",	"Wellness Disparity Profile",
            "Health Opportunity Index",], axis=1)
X_test_community = X_test.drop(["Access to Care", "Employment Accessibility",	"Affordability",
          	"Education",	"Food Accessibility",
            "Income Inequality",	"Job Participation",	"Population Density",
            "Segregation",	"Material Deprivation", "Consumer Opportunity Profile",
            "Economic Opportunity Profile",	"Wellness Disparity Profile",
            "Health Opportunity Index",], axis=1)

# ...Consumer Opportunity Profile: Affordability, Education, Food Accessibility, Material Deprivation
X_train_consumer = X_train.drop(["Access to Care", "Employment Accessibility",
            "Air Quality",	"Population Churning",
            "Income Inequality",	"Job Participation",	"Population Density",
            "Segregation",	"Walkability",
            "Community Environment Profile",
            "Economic Opportunity Profile",	"Wellness Disparity Profile",
            "Health Opportunity Index",], axis=1) 
X_test_consumer= X_test.drop(["Access to Care", "Employment Accessibility",
            "Air Quality",	"Population Churning",
            "Income Inequality",	"Job Participation",	"Population Density",
            "Segregation",	"Walkability",
            "Community Environment Profile",
            "Economic Opportunity Profile",	"Wellness Disparity Profile",
            "Health Opportunity Index",], axis=1) 


# ...Economic Opportunity Profile: Employment Accessibility, Income Inequality, Job Participation 
X_train_economic = X_train.drop(["Access to Care",	"Affordability",
            "Air Quality",	"Population Churning", "Education",	"Food Accessibility",
           	"Population Density",
            "Segregation",	"Material Deprivation",	"Walkability",
            "Community Environment Profile",	"Consumer Opportunity Profile",
            "Wellness Disparity Profile",
            "Health Opportunity Index",], axis=1) 
X_test_economic = X_test.drop(["Access to Care",	"Affordability",
            "Air Quality",	"Population Churning", "Education",	"Food Accessibility",
            "Population Density",
            "Segregation",	"Material Deprivation",	"Walkability",
            "Community Environment Profile",	"Consumer Opportunity Profile",
            "Wellness Disparity Profile",
            "Health Opportunity Index",], axis=1) 

# ...Wellness Disparity Profile: Access to Care, Segregation
X_train_wellness = X_train.drop(["Employment Accessibility",	"Affordability",
            "Air Quality",	"Population Churning", "Education",	"Food Accessibility",
            "Income Inequality",	"Job Participation",	"Population Density",
           	"Material Deprivation",	"Walkability",
            "Community Environment Profile",	"Consumer Opportunity Profile",
            "Economic Opportunity Profile",
            "Health Opportunity Index",], axis=1) 
X_test_wellness = X_test.drop(["Employment Accessibility",	"Affordability",
            "Air Quality",	"Population Churning", "Education",	"Food Accessibility",
            "Income Inequality",	"Job Participation",	"Population Density",
           	"Material Deprivation",	"Walkability",
            "Community Environment Profile",	"Consumer Opportunity Profile",
            "Economic Opportunity Profile",
            "Health Opportunity Index",], axis=1)

"""**Christine noté:**

I changed the label back to "deaths". I felt like with "deaths" as the unknown, we can better model that knowing the "active" or total cases, etc rather than vice versa. But like also we can tweak it later if we want to switch the label around. 

If the TA complains about our small datasets, I think have another idea that would require a bit more data cleaning but would let us use  more data (and if we can't find another dataset to augment what we already have as Sharon has previously suggested).

** I also didn't do visualizations so if one of you guys have extra time to do that it would great but no pressure.
"""

corr_drop_labels = ["active", "confirmed_delta", 
                    "active_delta", "deaths_delta", "long_",
                    "lat", "fips", "recovered", "recovered_delta"]
corr_data = data.drop(corr_drop_labels, axis = 1)
corr_matrix = corr_data.corr()
corr_matrix.shape
corr_matrix["deaths"].sort_values(ascending=False)

from pandas.plotting import scatter_matrix

# attributes = ["Food Accessibility", "Population Density", "Affordability",
#               "Job Participation", "Community Environment Profile", "Segregation",
#               "Education", "Material Deprivation", "Employment Accessibility",
#               "Air Quality", "Wellness Disparity Profile", "Walkability", 
#               "Economic Opportunity Profile", "Health Opportunity Index", "Income Inequality",
#               "Consumer Opportunity Profile", "Access to Care", "Population Churning"]
# scatter_matrix(corr_data[attributes], figsize=(20, 18))

"""# Selecting a model

Sharon - Try out different Regression models
"""

from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor

from sklearn.model_selection import cross_val_score

def display_scores(scores):
    print("Scores:", scores)
    print("Mean:", scores.mean())
    print("Standard deviation:", scores.std())

"""* note: still have to cross validate

Testing out the Linear Regression Model
"""

# No HOI data
X_train_noHOI_prepared = pipeline.fit_transform(X_train_noHOI)
X_test_noHOI_prepared = pipeline.fit_transform(X_test_noHOI)

lin_reg_noHOI = LinearRegression()
lin_reg_noHOI.fit(X_train_noHOI_prepared, y_train)

lin_reg_noHOI_predictions = lin_reg_noHOI.predict(X_test_noHOI_prepared)
lin_mse_noHOI = mean_squared_error(y_test, lin_reg_noHOI_predictions)
lin_rmse_noHOI = np.sqrt(lin_mse_noHOI)

lin_rmse_noHOI
lin_mae_noHOI = mean_absolute_error(y_test, lin_reg_noHOI_predictions)
lin_mae_noHOI

lin_scores_1 = cross_val_score(lin_reg_noHOI, X_test_noHOI_prepared, y_test,
                             scoring="neg_mean_squared_error", cv=10)
lin_rmse_scores_1 = np.sqrt(-lin_scores_1)
display_scores(lin_rmse_scores_1)

# Sets without all HOI features but the HOI calculation itself
X_train_health_prepared = pipeline.fit_transform(X_train_health)
X_test_health_prepared = pipeline.fit_transform(X_test_health)

lin_reg_health = LinearRegression()
lin_reg_health.fit(X_train_health_prepared, y_train)

lin_reg_health_predictions = lin_reg_health.predict(X_test_health_prepared)
lin_mse_health = mean_squared_error(y_test, lin_reg_health_predictions)
lin_rmse_health = np.sqrt(lin_mse_health)

lin_rmse_health

lin_mae_health = mean_absolute_error(y_test, lin_reg_health_predictions)
lin_mae_health

lin_scores_2 = cross_val_score(lin_reg_health, X_test_health_prepared, y_test,
                             scoring="neg_mean_squared_error", cv=10)
lin_rmse_scores_2 = np.sqrt(-lin_scores_2)
display_scores(lin_rmse_scores_2)

# Community Environment profile
X_train_community_prepared = pipeline.fit_transform(X_train_community)
X_test_community_prepared = pipeline.fit_transform(X_test_community)

lin_reg_community = LinearRegression()
lin_reg_community.fit(X_train_community_prepared, y_train)

lin_reg_community_predictions = lin_reg_community.predict(X_test_community_prepared)
lin_mse_community = mean_squared_error(y_test, lin_reg_community_predictions)
lin_rmse_community = np.sqrt(lin_mse_community)

lin_rmse_community

lin_mae_community = mean_absolute_error(y_test, lin_reg_community_predictions)
lin_mae_community

lin_scores_3 = cross_val_score(lin_reg_community, X_test_community_prepared, y_test,
                             scoring="neg_mean_squared_error", cv=10)
lin_rmse_scores_3 = np.sqrt(-lin_scores_3)
display_scores(lin_rmse_scores_3)

# Consumer Opportunity Profile
X_train_consumer_prepared = pipeline.fit_transform(X_train_consumer)
X_test_consumer_prepared = pipeline.fit_transform(X_test_consumer)

lin_reg_consumer = LinearRegression()
lin_reg_consumer.fit(X_train_consumer_prepared, y_train)

lin_reg_consumer_predictions = lin_reg_consumer.predict(X_test_consumer_prepared)
lin_mse_consumer = mean_squared_error(y_test, lin_reg_consumer_predictions)
lin_rmse_consumer = np.sqrt(lin_mse_consumer)

lin_rmse_consumer

lin_mae_consumer = mean_absolute_error(y_test, lin_reg_consumer_predictions)
lin_mae_consumer

# Economic Opportunity Profile
X_train_economic_prepared = pipeline.fit_transform(X_train_economic)
X_test_economic_prepared = pipeline.fit_transform(X_test_economic)

lin_reg_economic = LinearRegression()
lin_reg_economic.fit(X_train_economic_prepared, y_train)

lin_reg_economic_predictions = lin_reg_economic.predict(X_test_economic_prepared)
lin_mse_economic = mean_squared_error(y_test, lin_reg_economic_predictions)
lin_rmse_economic = np.sqrt(lin_mse_economic)

lin_rmse_economic

lin_mae_economic = mean_absolute_error(y_test, lin_reg_economic_predictions)
lin_mae_economic

# Wellness Disparity Profile
X_train_wellness_prepared = pipeline.fit_transform(X_train_wellness)
X_test_wellness_prepared = pipeline.fit_transform(X_test_wellness)

lin_reg_wellness = LinearRegression()
lin_reg_wellness.fit(X_train_wellness_prepared, y_train)

lin_reg_wellness_predictions = lin_reg_wellness.predict(X_test_wellness_prepared)
lin_mse_wellness = mean_squared_error(y_test, lin_reg_wellness_predictions)
lin_rmse_wellness = np.sqrt(lin_mse_wellness)

lin_rmse_wellness

lin_mae_wellness = mean_absolute_error(y_test, lin_reg_wellness_predictions)
lin_mae_wellness

lin_scores_4 = cross_val_score(lin_reg_wellness, X_test_wellness_prepared, y_test,
                             scoring="neg_mean_squared_error", cv=10)
lin_rmse_scores_4 = np.sqrt(-lin_scores_4)
display_scores(lin_rmse_scores_4)

"""Testing out the Decision Tree Regressor"""

# No HOI data
tree_reg_noHOI = DecisionTreeRegressor(random_state=42)
tree_reg_noHOI.fit(X_train_noHOI_prepared, y_train)

tree_reg_noHOI_predictions = tree_reg_noHOI.predict(X_test_noHOI_prepared)
tree_mse_noHOI = mean_squared_error(y_test, tree_reg_noHOI_predictions)
tree_rmse_noHOI = np.sqrt(tree_mse_noHOI)
tree_rmse_noHOI

lin_scores_tree = cross_val_score(tree_reg_noHOI, X_test_noHOI_prepared, y_test,
                             scoring="neg_mean_squared_error", cv=5)
lin_rmse_scores_tree = np.sqrt(-lin_scores_tree)
display_scores(lin_rmse_scores_tree)

# Sets without all HOI features but the HOI calculation itself
tree_reg_health = DecisionTreeRegressor(random_state=42)
tree_reg_health.fit(X_train_health_prepared, y_train)

tree_reg_health_predictions = tree_reg_health.predict(X_test_health_prepared)
tree_mse_health = mean_squared_error(y_test, tree_reg_health_predictions)
tree_rmse_health = np.sqrt(tree_mse_health)
tree_rmse_health

# Community Environment profile
tree_reg_community = DecisionTreeRegressor(random_state=42)
tree_reg_community.fit(X_train_community_prepared, y_train)

tree_reg_community_predictions = tree_reg_community.predict(X_test_community_prepared)
tree_mse_community = mean_squared_error(y_test, tree_reg_community_predictions)
tree_rmse_community = np.sqrt(tree_mse_community)
tree_rmse_community

# Consumer Opportunity Profile
tree_reg_consumer = DecisionTreeRegressor(random_state=42)
tree_reg_consumer.fit(X_train_consumer_prepared, y_train)

tree_reg_consumer_predictions = tree_reg_consumer.predict(X_test_consumer_prepared)
tree_mse_consumer = mean_squared_error(y_test, tree_reg_consumer_predictions)
tree_rmse_consumer = np.sqrt(tree_mse_consumer)
tree_rmse_consumer

# Economic Opportunity Profile
tree_reg_economic = DecisionTreeRegressor(random_state=42)
tree_reg_economic.fit(X_train_economic_prepared, y_train)

tree_reg_economic_predictions = tree_reg_economic.predict(X_test_economic_prepared)
tree_mse_economic = mean_squared_error(y_test, tree_reg_economic_predictions)
tree_rmse_economic = np.sqrt(tree_mse_economic)
tree_rmse_economic

# Wellness Disparity Profile
tree_reg_wellness = DecisionTreeRegressor(random_state=42)
tree_reg_wellness.fit(X_train_wellness_prepared, y_train)

tree_reg_wellness_predictions = tree_reg_wellness.predict(X_test_wellness_prepared)
tree_mse_wellness = mean_squared_error(y_test, tree_reg_wellness_predictions)
tree_rmse_wellness = np.sqrt(tree_mse_wellness)
tree_rmse_wellness

"""Testing out the Random Forest Regressor"""

# No HOI data
forest_reg_noHOI = RandomForestRegressor(n_estimators=100, random_state=42)
forest_reg_noHOI.fit(X_train_noHOI_prepared, y_train)

forest_reg_noHOI_predictions = forest_reg_noHOI.predict(X_test_noHOI_prepared)
forest_mse_noHOI = mean_squared_error(y_test, forest_reg_noHOI_predictions)
forest_rmse_noHOI = np.sqrt(forest_mse_noHOI)
forest_rmse_noHOI

lin_scores_forest = cross_val_score(forest_reg_noHOI, X_test_noHOI_prepared, y_test,
                             scoring="neg_mean_squared_error", cv=10)
lin_rmse_scores_forest = np.sqrt(-lin_scores_forest)
display_scores(lin_rmse_scores_forest)

# Sets without all HOI features but the HOI calculation itself
forest_reg_health = RandomForestRegressor(n_estimators=100, random_state=42)
forest_reg_health.fit(X_train_health_prepared, y_train)

forest_reg_health_predictions = forest_reg_health.predict(X_test_health_prepared)
forest_mse_health = mean_squared_error(y_test, forest_reg_health_predictions)
forest_rmse_health = np.sqrt(forest_mse_health)
forest_rmse_health

lin_scores_forest_1 = cross_val_score(forest_reg_health, X_test_health_prepared, y_test,
                             scoring="neg_mean_squared_error", cv=10)
lin_rmse_scores_forest_1 = np.sqrt(-lin_scores_forest_1)
display_scores(lin_rmse_scores_forest_1)

forest_reg_health.get_params

# Community Environment profile
forest_reg_community = RandomForestRegressor(n_estimators=100, random_state=42)
forest_reg_community.fit(X_train_community_prepared, y_train)

forest_reg_community_predictions = forest_reg_community.predict(X_test_community_prepared)
forest_mse_community = mean_squared_error(y_test, forest_reg_community_predictions)
forest_rmse_community = np.sqrt(forest_mse_community)
forest_rmse_community



# Consumer Opportunity Profile
forest_reg_consumer = RandomForestRegressor(n_estimators=100, random_state=42)
forest_reg_consumer.fit(X_train_consumer_prepared, y_train)

forest_reg_consumer_predictions = forest_reg_consumer.predict(X_test_consumer_prepared)
forest_mse_consumer = mean_squared_error(y_test, forest_reg_consumer_predictions)
forest_rmse_consumer = np.sqrt(forest_mse_consumer)
forest_rmse_consumer

lin_scores_forest_2 = cross_val_score(forest_reg_consumer, X_test_consumer_prepared, y_test,
                             scoring="neg_mean_squared_error", cv=10)
lin_rmse_scores_forest_2 = np.sqrt(-lin_scores_forest_2)
display_scores(lin_rmse_scores_forest_2)

# Economic Opportunity Profile
forest_reg_economic = RandomForestRegressor(n_estimators=100, random_state=42)
forest_reg_economic.fit(X_train_economic_prepared, y_train)

forest_reg_economic_predictions = forest_reg_economic.predict(X_test_economic_prepared)
forest_mse_economic = mean_squared_error(y_test, forest_reg_economic_predictions)
forest_rmse_economic = np.sqrt(forest_mse_economic)
forest_rmse_economic

lin_scores_forest_3 = cross_val_score(forest_reg_economic, X_test_economic_prepared, y_test,
                             scoring="neg_mean_squared_error", cv=10)
lin_rmse_scores_forest_3 = np.sqrt(-lin_scores_forest_3)
display_scores(lin_rmse_scores_forest_3)

# Wellness Disparity Profile
forest_reg_wellness = RandomForestRegressor(n_estimators=100, random_state=42)
forest_reg_wellness.fit(X_train_wellness_prepared, y_train)

forest_reg_wellness_predictions = forest_reg_wellness.predict(X_test_wellness_prepared)
forest_mse_wellness = mean_squared_error(y_test, forest_reg_wellness_predictions)
forest_rmse_wellness = np.sqrt(forest_mse_wellness)
forest_rmse_wellness

lin_scores_forest_4 = cross_val_score(forest_reg_wellness, X_test_wellness_prepared, y_test,
                             scoring="neg_mean_squared_error", cv=10)
lin_rmse_scores_forest_4 = np.sqrt(-lin_scores_forest_4)
display_scores(lin_rmse_scores_forest_4)

"""# Tuning the model

Jimmy - Tune the regression model we end up picking

From the cross validation, I'm thinking that we do forest_reg_health and x_test_health
"""

from sklearn.model_selection import GridSearchCV

param_grid = [
    # try 12 (3×4) combinations of hyperparameters
    {'n_estimators': [3, 10, 30, 50, 100, 250, 180, 200], 'max_features': [2, 3, 4, 5, 6, 7, 8, 9, 10]},
    # then try 6 (2×3) combinations with bootstrap set as False
    {'bootstrap': [False], 'n_estimators': [3, 10, 30, 50, 100, 250, 180, 200], 'max_features': [2, 3, 4, 5, 6, 7, 8, 9, 10]},
  ]

  
# train across 5 folds, that's a total of 90 rounds of training 
grid_search = GridSearchCV(forest_reg_health, param_grid, cv=5,
                           scoring='neg_mean_squared_error',
                           return_train_score=True)
grid_search.fit(X_train_health_prepared, y_train)



final_model = grid_search.best_estimator_

final_scores = cross_val_score(final_model, X_test_wellness_prepared, y_test,
                             scoring="neg_mean_squared_error", cv=10)
final_rmse_scores = np.sqrt(-final_scores)
display_scores(final_rmse_scores)

X_train_health.head()

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint

param_distribs = {
        'n_estimators': randint(low=1, high=200),
        'max_features': ['auto', 'sqrt'],
    }

# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 10, stop = 2000, num = 10)]
# Number of features to consider at every split
max_features = ['auto', 'sqrt']
# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
max_depth.append(None)
# Minimum number of samples required to split a node
min_samples_split = [2, 5, 10]
# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2, 4]
# Method of selecting samples for training each tree
bootstrap = [True, False]

random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}

#forest_reg = RandomForestRegressor(random_state=42)
rnd_search = RandomizedSearchCV(forest_reg_health, param_distributions=param_distribs,
                                n_iter=10, cv=5, scoring='neg_mean_squared_error', random_state=42)
rnd_search.fit(X_train_health_prepared, y_train)

cvres = rnd_search.cv_results_
for mean_score, params in zip(cvres["mean_test_score"], cvres["params"]):
    print(np.sqrt(-mean_score), params)

final_rnd_model = rnd_search.best_estimator_

final_rnd_scores = cross_val_score(final_rnd_model, X_test_wellness_prepared, y_test,
                             scoring="neg_mean_squared_error", cv=10)
final_rnd_rmse_scores = np.sqrt(-final_rnd_scores)
display_scores(final_rnd_rmse_scores)

"""# Presenting Our Solution

What we found

"""